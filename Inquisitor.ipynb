{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Mw6HXDpMVP",
        "outputId": "0e22234e-0a63-4a52-fe53-13fcf39e118e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertTokenizer\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Extract contexts and questions from the dataset\n",
        "contexts = [item['context'] for item in dataset['train']]\n",
        "questions = [item['question'] for item in dataset['train']]\n",
        "\n",
        "# Display the first few contexts and questions to verify\n",
        "for i in range(3):\n",
        "    print(f\"Context {i}: {contexts[i]}\")\n",
        "    print(f\"Question {i}: {questions[i]}\\n\")\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Create a custom Dataset class for Question Generation\n",
        "class QuestionGenerationDataset(Dataset):\n",
        "    def __init__(self, contexts, questions, tokenizer, max_length=512):\n",
        "        self.contexts = contexts\n",
        "        self.questions = questions\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.contexts[idx]\n",
        "        question = self.questions[idx]\n",
        "\n",
        "        # Tokenize context\n",
        "        inputs = self.tokenizer(\n",
        "            context,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Tokenize question\n",
        "        question_inputs = self.tokenizer(\n",
        "            question,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),  # Remove the batch dimension\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "            'labels': question_inputs['input_ids'].squeeze(0)  # Output as labels for training\n",
        "        }\n",
        "\n",
        "# Create the dataset\n",
        "qg_dataset = QuestionGenerationDataset(contexts[:100], questions[:100], tokenizer)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(qg_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Define the LSTM-based Question Generation model\n",
        "class LSTMQuestionGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMQuestionGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, hidden):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output, (hidden, cell)\n",
        "\n",
        "# Initialize model parameters\n",
        "vocab_size = tokenizer.vocab_size\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = vocab_size  # Same as vocab size for output layer\n",
        "\n",
        "# Create the model\n",
        "model = LSTMQuestionGenerator(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # Initialize hidden state\n",
        "        hidden = (torch.zeros(1, input_ids.size(0), hidden_dim),\n",
        "                  torch.zeros(1, input_ids.size(0), hidden_dim))\n",
        "\n",
        "        # Forward pass\n",
        "        outputs, hidden = model(input_ids, hidden)\n",
        "        loss = criterion(outputs.view(-1, output_dim), labels.view(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Function to generate questions\n",
        "def generate_question(model, context, tokenizer, max_length=30, temperature=1.0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize context\n",
        "        inputs = tokenizer(context, return_tensors='pt', padding='longest', truncation=True, max_length=512)\n",
        "        input_ids = inputs['input_ids']  # Shape: (1, seq_length)\n",
        "\n",
        "        # Initialize hidden state\n",
        "        hidden = (torch.zeros(1, input_ids.size(0), hidden_dim),\n",
        "                  torch.zeros(1, input_ids.size(0), hidden_dim))\n",
        "\n",
        "        # Generate output tokens\n",
        "        generated = []\n",
        "        for _ in range(max_length):\n",
        "            output, hidden = model(input_ids, hidden)\n",
        "\n",
        "            # Sample from the distribution with temperature\n",
        "            output_dist = output[:, -1, :] / temperature\n",
        "            next_token = torch.multinomial(F.softmax(output_dist, dim=-1), num_samples=1)\n",
        "\n",
        "            # Ensure next_token is 2D for concatenation\n",
        "            next_token = next_token.squeeze(1)  # Shape: (1,)\n",
        "\n",
        "            # Append token to generated list\n",
        "            generated.append(next_token.item())\n",
        "            input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)  # Shape: (1, seq_length + 1)\n",
        "\n",
        "            # Break if end token is generated\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Convert token IDs to question\n",
        "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
        "\n",
        "# Function to evaluate generated questions\n",
        "def evaluate_question_generation(predicted, actual):\n",
        "    # Tokenize predicted and actual questions\n",
        "    predicted_tokens = predicted.split()\n",
        "    actual_tokens = actual.split()\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    score = sentence_bleu([actual_tokens], predicted_tokens)\n",
        "    return score\n",
        "\n",
        "# Example usage\n",
        "context_example = contexts[0]  # Example context\n",
        "actual_question = questions[0]  # Actual question\n",
        "predicted_question = generate_question(model, context_example, tokenizer)\n",
        "bleu_score = evaluate_question_generation(predicted_question, actual_question)\n",
        "\n",
        "print(\"Predicted Question:\", predicted_question)\n",
        "print(\"Actual Question:\", actual_question)\n",
        "print(\"BLEU Score:\", bleu_score)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
